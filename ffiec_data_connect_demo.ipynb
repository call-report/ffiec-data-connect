{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFIEC Data Connect - Comprehensive Demo\n",
    "\n",
    "This notebook demonstrates all the capabilities of the FFIEC Data Connect library, including:\n",
    "- Basic data collection (sync)\n",
    "- Async data collection with rate limiting\n",
    "- Parallel processing for multiple data requests\n",
    "- Working with pandas and polars DataFrames\n",
    "- Error handling and validation\n",
    "- Performance comparisons\n",
    "\n",
    "**Note**: You'll need valid FFIEC credentials to run the live data examples. Mock examples are provided for demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFIEC Data Connect - Comprehensive Demo\n",
    "\n",
    "This notebook demonstrates all the capabilities of the FFIEC Data Connect library, including:\n",
    "- Basic data collection (sync)\n",
    "- Async data collection with rate limiting\n",
    "- Parallel processing for multiple data requests\n",
    "- Working with pandas and polars DataFrames\n",
    "- Error handling and validation\n",
    "- Performance comparisons\n",
    "\n",
    "## üöÄ Quick Start\n",
    "\n",
    "1. **First**: Run the installation cell above to install all required packages\n",
    "2. **Second**: Enter your FFIEC credentials when prompted (or set environment variables)\n",
    "3. **Then**: Run all remaining cells to see the full demonstration\n",
    "\n",
    "## üìã Requirements\n",
    "\n",
    "- Python 3.8+\n",
    "- Valid FFIEC CDR credentials (or demo mode will be used)\n",
    "- Internet connection for package installation\n",
    "\n",
    "**Note**: If you have FFIEC credentials, the notebook will collect real data. Otherwise, it will demonstrate all features using mock data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Standard library imports\nimport os\nimport asyncio\nimport time\nimport random\nfrom datetime import datetime, timedelta\nfrom typing import List, Dict, Any\n\n# Third-party imports\nimport pandas as pd\nimport polars as pl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# FFIEC Data Connect imports\nimport ffiec_data_connect as fdc\nfrom ffiec_data_connect import (\n    WebserviceCredentials,\n    FFIECConnection,\n    AsyncCompatibleClient,\n    RateLimiter,\n    collect_data,\n    collect_reporting_periods,\n    collect_filers_on_reporting_period,\n    collect_filers_since_date,\n    collect_filers_submission_date_time,\n    CredentialError,\n    ValidationError,\n    NoDataError\n)\n\nprint(f\"FFIEC Data Connect version: {fdc.__version__}\")\nprint(f\"Pandas version: {pd.__version__}\")\nprint(f\"Polars version: {pl.__version__}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell removed - AsyncCompatibleClient will be created later in the connection management section"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Interactive credential input\nprint(\"üîê FFIEC CDR Credentials\")\nprint(\"Enter your credentials:\")\n\nimport getpass\n\nusername = input(\"Username: \").strip()\npassword = getpass.getpass(\"Password: \")\n\ncredentials = WebserviceCredentials(username, password)\nprint(f\"‚úÖ Credentials set for: {credentials.username}\")\n\n# Alternative: Environment variables (uncomment to use instead)\n# credentials = WebserviceCredentials()  # Uses FFIEC_USERNAME and FFIEC_PASSWORD env vars",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## üîå Connection Management\n\nThe library provides multiple connection management approaches:\n- Basic FFIECConnection\n- AsyncCompatibleClient with advanced features\n- Context managers for automatic cleanup"
  },
  {
   "cell_type": "code",
   "source": "# Basic connection\nbasic_connection = FFIECConnection()\nprint(f\"Basic connection: {basic_connection}\")\nprint(f\"Session active: {basic_connection.session is not None}\")\n\n# Advanced async-compatible client with rate limiting\nasync_client = AsyncCompatibleClient(\n    credentials=credentials,\n    max_concurrent=4,  # Max 4 concurrent requests\n    rate_limit=0.5  # Max 0.5 requests per second (30 per minute)\n)\n\nprint(f\"\\nAsync client: {async_client}\")\nprint(f\"Rate limiter: {async_client.rate_limiter}\")\n\n# Context manager usage (recommended)\nprint(\"\\nüîÑ Testing context manager...\")\nwith AsyncCompatibleClient(credentials) as client:\n    print(f\"Client active: {len(client._connection_cache) >= 0}\")  # Check cache exists\nprint(\"Client automatically closed after context\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Basic Data Collection (Synchronous)\n",
    "\n",
    "Start with the traditional synchronous API for collecting FFIEC data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Sample data for examples\nSAMPLE_BANKS = [\"480228\", \"852320\"]  # JPMorgan Chase, Bank of America  \nSAMPLE_PERIODS = [\"2023-12-31\", \"2023-09-30\"]\n\nprint(f\"Sample banks (RSSD IDs): {SAMPLE_BANKS}\")\nprint(f\"Sample periods: {SAMPLE_PERIODS}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get available reporting periods\n",
    "print(\"üìÖ Getting available reporting periods...\")\n",
    "\n",
    "try:\n",
    "    periods = collect_reporting_periods(\n",
    "        session=basic_connection.session,\n",
    "        creds=credentials,\n",
    "        series=\"call\",\n",
    "        output_type=\"list\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Found {len(periods)} reporting periods\")\n",
    "    print(\"Recent periods:\")\n",
    "    for period in periods[:5]:  # Show first 5\n",
    "        print(f\"  - {period}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error getting periods: {e}\")\n",
    "    periods = SAMPLE_PERIODS  # Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Collect data for a single bank\n",
    "print(\"üè¶ Collecting data for a single bank...\")\n",
    "\n",
    "rssd_id = SAMPLE_BANKS[0]  # JPMorgan Chase\n",
    "reporting_period = SAMPLE_PERIODS[0]  # Most recent\n",
    "\n",
    "print(f\"Bank RSSD ID: {rssd_id}\")\n",
    "print(f\"Reporting period: {reporting_period}\")\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    bank_data = collect_data(\n",
    "        session=basic_connection.session,\n",
    "        creds=credentials,\n",
    "        reporting_period=reporting_period,\n",
    "        rssd_id=rssd_id,\n",
    "        series=\"call\",\n",
    "        output_type=\"list\"\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ Collected {len(bank_data)} data points in {elapsed:.2f} seconds\")\n",
    "    \n",
    "    # Show sample data\n",
    "    if bank_data:\n",
    "        print(\"\\nSample data points:\")\n",
    "        for i, item in enumerate(bank_data[:3]):\n",
    "            print(f\"  {i+1}. {item}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error collecting data: {e}\")\n",
    "    bank_data = []"
   ]
  },
  {
   "cell_type": "code",
   "source": "# 5. Get detailed submission timestamps for banks\nprint(\"‚è∞ Getting bank submission timestamps...\")\n\ntry:\n    # Use the same date range as above\n    submission_info = collect_filers_submission_date_time(\n        session=basic_connection.session,\n        creds=credentials,\n        since_date=since_date_str,\n        reporting_period=SAMPLE_PERIODS[0],\n        output_type=\"pandas\"  # This returns detailed timestamp data\n    )\n    \n    if isinstance(submission_info, pd.DataFrame) and len(submission_info) > 0:\n        print(f\"‚úÖ Found submission details for {len(submission_info)} filings\")\n        print(f\"Columns: {submission_info.columns.tolist()}\")\n        \n        # Show sample submission data\n        print(\"\\nSample submission timestamps:\")\n        print(submission_info.head())\n        \n        # Summary statistics if we have timestamp data\n        if 'submission_date' in submission_info.columns or any('date' in col.lower() for col in submission_info.columns):\n            print(\"\\nüìä Submission timing analysis:\")\n            date_cols = [col for col in submission_info.columns if 'date' in col.lower() or 'time' in col.lower()]\n            if date_cols:\n                print(f\"Date/time columns available: {date_cols}\")\n    else:\n        print(f\"‚úÖ Got submission info: {type(submission_info)} with {len(submission_info) if hasattr(submission_info, '__len__') else 'N/A'} items\")\n        if hasattr(submission_info, '__iter__') and len(str(submission_info)) < 500:\n            print(f\"Sample data: {list(submission_info)[:3] if hasattr(submission_info, '__iter__') else submission_info}\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Error getting submission timestamps: {e}\")\n    submission_info = []",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 4. Get banks that have filed since a specific date\nprint(\"üìÖ Getting banks that filed since a specific date...\")\n\n# Use a date 30 days before the reporting period\nfrom datetime import datetime, timedelta\ntry:\n    reporting_date = datetime.strptime(SAMPLE_PERIODS[0], \"%Y-%m-%d\")\n    since_date = reporting_date - timedelta(days=30)\n    since_date_str = since_date.strftime(\"%Y-%m-%d\")\n    \n    print(f\"Looking for filings since: {since_date_str}\")\n    \n    recent_filers = collect_filers_since_date(\n        session=basic_connection.session,\n        creds=credentials,\n        reporting_period=SAMPLE_PERIODS[0],\n        since_date=since_date_str,\n        output_type=\"list\"\n    )\n    \n    print(f\"‚úÖ Found {len(recent_filers)} banks that filed since {since_date_str}\")\n    \n    # Show sample recent filers\n    if recent_filers:\n        print(\"\\nRecent filers (sample):\")\n        for i, filer in enumerate(recent_filers[:5]):  # Show first 5\n            print(f\"  {i+1}. RSSD ID: {filer}\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Error getting recent filers: {e}\")\n    recent_filers = []",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 3. Get all banks that filed for a specific reporting period\nprint(\"üè¢ Getting all filers for a reporting period...\")\n\ntry:\n    filers = collect_filers_on_reporting_period(\n        session=basic_connection.session,\n        creds=credentials,\n        reporting_period=SAMPLE_PERIODS[0],\n        output_type=\"list\"\n    )\n    \n    print(f\"‚úÖ Found {len(filers)} banks that filed for {SAMPLE_PERIODS[0]}\")\n    \n    # Show sample filers\n    if filers:\n        print(\"\\nSample filers:\")\n        for i, filer in enumerate(filers[:5]):  # Show first 5\n            print(f\"  {i+1}. RSSD ID: {filer}\")\n    \n    # Store for later use\n    available_filers = filers[:10] if filers else SAMPLE_BANKS\n    \nexcept Exception as e:\n    print(f\"‚ùå Error getting filers: {e}\")\n    available_filers = SAMPLE_BANKS",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## üè¢ Additional Collection Methods\n\nThe library provides several specialized methods for getting information about bank filings and submissions.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Async Data Collection\n",
    "\n",
    "The AsyncCompatibleClient provides async methods for better performance when collecting multiple datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Async data collection with rate limiting\n",
    "async def collect_data_async_demo():\n",
    "    \"\"\"Demo async data collection with rate limiting.\"\"\"\n",
    "    \n",
    "    # Use async context manager\n",
    "    async with AsyncCompatibleClient(\n",
    "        credentials=credentials,\n",
    "        max_concurrent=5,\n",
    "        rate_limit=2.0  # 2 requests per second\n",
    "    ) as client:\n",
    "        \n",
    "        print(\"üöÄ Starting async data collection...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Collect data for multiple banks async\n",
    "        tasks = []\n",
    "        for rssd_id in SAMPLE_BANKS[:2]:  # First 2 banks\n",
    "            task = client.collect_data_async(\n",
    "                reporting_period=SAMPLE_PERIODS[0],\n",
    "                rssd_id=rssd_id,\n",
    "                series=\"call\"\n",
    "            )\n",
    "            tasks.append((rssd_id, task))\n",
    "        \n",
    "        # Wait for all tasks to complete\n",
    "        results = []\n",
    "        for rssd_id, task in tasks:\n",
    "            try:\n",
    "                data = await task\n",
    "                results.append((rssd_id, data))\n",
    "                print(f\"‚úÖ Bank {rssd_id}: {len(data)} data points\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Bank {rssd_id}: Error - {e}\")\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n‚è±Ô∏è Async collection completed in {elapsed:.2f} seconds\")\n",
    "        return results\n",
    "\n",
    "# Run the async demo\n",
    "try:\n",
    "    async_results = await collect_data_async_demo()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Async demo error: {e}\")\n",
    "    async_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Parallel Processing (Sync Interface)\n",
    "\n",
    "For users who prefer synchronous code, the library provides parallel processing with a sync interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel data collection with sync interface\n",
    "print(\"üîÑ Parallel data collection demo...\")\n",
    "\n",
    "with AsyncCompatibleClient(credentials, max_concurrent=3) as client:\n",
    "    \n",
    "    print(f\"Collecting data for multiple banks in parallel...\")\n",
    "    \n",
    "    # Progress callback function\n",
    "    def progress_callback(rssd_id: str, result: Any):\n",
    "        if 'error' in str(result):\n",
    "            print(f\"‚ùå Bank {rssd_id}: Error\")\n",
    "        else:\n",
    "            data_points = len(result) if isinstance(result, list) else 0\n",
    "            print(f\"‚úÖ Bank {rssd_id}: {data_points} data points\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Use parallel collection method with correct signature\n",
    "        results = client.collect_data_parallel(\n",
    "            reporting_period=SAMPLE_PERIODS[0],  # Single period\n",
    "            rssd_ids=SAMPLE_BANKS[:2],  # Multiple banks\n",
    "            series='call',\n",
    "            progress_callback=progress_callback\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        # Process results\n",
    "        successful = [r for r in results.values() if 'error' not in str(r)]\n",
    "        failed = [r for r in results.values() if 'error' in str(r)]\n",
    "        \n",
    "        print(f\"\\n‚úÖ Parallel collection completed in {elapsed:.2f} seconds\")\n",
    "        print(f\"üìà Successful: {len(successful)}, Failed: {len(failed)}\")\n",
    "        \n",
    "        if successful:\n",
    "            total_data_points = sum(len(r) if isinstance(r, list) else 0 for r in successful)\n",
    "            print(f\"üìä Total data points collected: {total_data_points}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Parallel collection error: {e}\")\n",
    "        results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Type Consistency with Polars\n",
    "\n",
    "The library ensures proper numpy dtypes throughout the data pipeline from XBRL processing to pandas DataFrames to polars conversion. Let's convert the pandas DataFrame to polars and examine the type consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct XBRL to polars conversion - preserves maximum precision\n",
    "try:\n",
    "    df_polars_direct = collect_data(\n",
    "        session=basic_connection.session,\n",
    "        creds=credentials, \n",
    "        reporting_period=\"2023-12-31\",\n",
    "        rssd_id=\"480228\",\n",
    "        series=\"call\",\n",
    "        output_type=\"polars\"  # Direct conversion from XBRL to polars\n",
    "    )\n",
    "    \n",
    "    print(\"Direct XBRL ‚Üí polars DataFrame:\")\n",
    "    print(f\"Shape: {df_polars_direct.shape}\")\n",
    "    print(f\"Schema: {df_polars_direct.schema}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df_polars_direct.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating polars DataFrame: {e}\")\n",
    "    # Create empty DataFrame for demo\n",
    "    import polars as pl\n",
    "    df_polars_direct = pl.DataFrame({\n",
    "        'mdrm': ['DEMO'],\n",
    "        'rssd': ['480228'], \n",
    "        'quarter': ['2023-12-31'],\n",
    "        'data_type': ['int'],\n",
    "        'int_data': [1000000],\n",
    "        'float_data': [None],\n",
    "        'bool_data': [None],\n",
    "        'str_data': [None]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas DataFrame for comparison\n",
    "try:\n",
    "    df_pandas = collect_data(\n",
    "        session=basic_connection.session,\n",
    "        creds=credentials,\n",
    "        reporting_period=\"2023-12-31\", \n",
    "        rssd_id=\"480228\",\n",
    "        series=\"call\",\n",
    "        output_type=\"pandas\"  # Standard pandas conversion\n",
    "    )\n",
    "    \n",
    "    print(\"=== Type Precision Comparison ===\")\n",
    "    print(\"\\nPandas DataFrame (via pandas conversion):\")\n",
    "    print(df_pandas.dtypes)\n",
    "    \n",
    "    if len(df_pandas) > 0 and 'int_data' in df_pandas.columns:\n",
    "        int_samples = df_pandas.dropna(subset=['int_data'])\n",
    "        if len(int_samples) > 0:\n",
    "            sample_pandas = int_samples['int_data'].iloc[0]\n",
    "            print(f\"Sample int value: {sample_pandas} (type: {type(sample_pandas)})\")\n",
    "        \n",
    "        print(\"\\nPolars DataFrame (direct from XBRL):\")  \n",
    "        print(df_polars_direct.schema)\n",
    "        \n",
    "        if len(df_polars_direct) > 0:\n",
    "            polars_ints = df_polars_direct.filter(pl.col('int_data').is_not_null())\n",
    "            if len(polars_ints) > 0:\n",
    "                sample_polars = polars_ints['int_data'].first()\n",
    "                print(f\"Sample int value: {sample_polars} (type: {type(sample_polars)})\")\n",
    "                \n",
    "                # Verify no precision loss in the direct conversion\n",
    "                int_data_polars = polars_ints['int_data'].to_list()\n",
    "                int_data_pandas = int_samples['int_data'].tolist()\n",
    "                \n",
    "                print(f\"\\nPrecision check - values match: {int_data_polars == int_data_pandas}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in comparison: {e}\")\n",
    "    # Create demo pandas DataFrame\n",
    "    import pandas as pd\n",
    "    df_pandas = pd.DataFrame({\n",
    "        'mdrm': ['DEMO'],\n",
    "        'rssd': ['480228'],\n",
    "        'quarter': ['2023-12-31'], \n",
    "        'data_type': ['int'],\n",
    "        'int_data': [1000000],\n",
    "        'float_data': [None],\n",
    "        'bool_data': [None],\n",
    "        'str_data': [None]\n",
    "    })\n",
    "    print(\"Using demo data for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pandas data to various formats\n",
    "print(\"üíæ Saving pandas data to files...\")\n",
    "\n",
    "# Save to CSV\n",
    "csv_file = 'ffiec_data_pandas.csv'\n",
    "df_pandas.to_csv(csv_file, index=False)\n",
    "print(f\"‚úÖ Saved to CSV: {csv_file}\")\n",
    "\n",
    "# Save to Excel\n",
    "excel_file = 'ffiec_data_pandas.xlsx'\n",
    "with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n",
    "    df_pandas.to_excel(writer, sheet_name='All_Data', index=False)\n",
    "    \n",
    "    # Create separate sheets by metric\n",
    "    if 'metric' in df_pandas.columns:\n",
    "        for metric in df_pandas['metric'].unique():\n",
    "            metric_data = df_pandas[df_pandas['metric'] == metric]\n",
    "            safe_sheet_name = metric.replace(' ', '_')[:31]  # Excel sheet name limits\n",
    "            metric_data.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
    "\n",
    "print(f\"‚úÖ Saved to Excel: {excel_file}\")\n",
    "\n",
    "# Save to Parquet (efficient for large datasets)\n",
    "parquet_file = 'ffiec_data_pandas.parquet'\n",
    "df_pandas.to_parquet(parquet_file, index=False)\n",
    "print(f\"‚úÖ Saved to Parquet: {parquet_file}\")\n",
    "\n",
    "# Display file sizes\n",
    "import os\n",
    "print(\"\\nüìè File sizes:\")\n",
    "for file in [csv_file, excel_file, parquet_file]:\n",
    "    if os.path.exists(file):\n",
    "        size_mb = os.path.getsize(file) / (1024 * 1024)\n",
    "        print(f\"  {file}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the schemas to verify type consistency\n",
    "print(\"=== Final Schema Comparison ===\")\n",
    "\n",
    "if 'df_pandas' in locals():\n",
    "    print(\"\\nPandas DataFrame dtypes:\")\n",
    "    print(df_pandas.dtypes)\n",
    "else:\n",
    "    print(\"\\n‚ùå Pandas DataFrame not available\")\n",
    "\n",
    "if 'df_polars_direct' in locals():\n",
    "    print(\"\\nPolars DataFrame schema (direct conversion):\")\n",
    "    print(df_polars_direct.schema)\n",
    "    \n",
    "    # Show some sample data with proper types\n",
    "    if len(df_polars_direct) > 0:\n",
    "        int_vals = df_polars_direct.filter(pl.col('int_data').is_not_null())\n",
    "        float_vals = df_polars_direct.filter(pl.col('float_data').is_not_null())\n",
    "        bool_vals = df_polars_direct.filter(pl.col('bool_data').is_not_null())\n",
    "        \n",
    "        if len(int_vals) > 0:\n",
    "            sample_int = int_vals['int_data'].first()\n",
    "            print(f\"\\nSample integer value: {sample_int} (type: {type(sample_int)})\")\n",
    "            \n",
    "        if len(float_vals) > 0:\n",
    "            sample_float = float_vals['float_data'].first()\n",
    "            print(f\"Sample float value: {sample_float} (type: {type(sample_float)})\")\n",
    "            \n",
    "        if len(bool_vals) > 0:\n",
    "            sample_bool = bool_vals['bool_data'].first()\n",
    "            print(f\"Sample boolean value: {sample_bool} (type: {type(sample_bool)})\")\n",
    "        else:\n",
    "            print(\"\\nNo boolean data in this dataset\")\n",
    "            \n",
    "    print(f\"\\n‚úÖ Direct polars conversion preserves {len(df_polars_direct.schema)} column types\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Polars DataFrame not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Polars DataFrame from the same data\n",
    "print(\"‚ö° Creating Polars DataFrame...\")\n",
    "\n",
    "# Convert from pandas or create directly\n",
    "df_polars = pl.from_pandas(df_pandas)\n",
    "\n",
    "# Or create directly from data\n",
    "# df_polars = pl.DataFrame(pandas_data)\n",
    "\n",
    "print(f\"Created Polars DataFrame with {len(df_polars)} rows\")\n",
    "print(f\"Columns: {df_polars.columns}\")\n",
    "print(f\"Schema: {df_polars.schema}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nüìä Sample data (Polars):\")\n",
    "print(df_polars.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars data analysis examples\n",
    "print(\"‚ö° Polars Analysis Examples:\")\n",
    "\n",
    "# Check the actual data structure to provide meaningful analysis\n",
    "print(f\"\\nAvailable columns: {df_polars.columns}\")\n",
    "print(f\"Data types in the data: {df_polars['data_type'].unique().to_list() if 'data_type' in df_polars.columns else 'No data_type column'}\")\n",
    "\n",
    "# Create meaningful analysis based on the actual data structure\n",
    "if len(df_polars) > 0 and 'int_data' in df_polars.columns:\n",
    "    \n",
    "    # 1. Basic aggregation by data type\n",
    "    print(\"\\nüìä Data summary by type:\")\n",
    "    type_summary = (\n",
    "        df_polars\n",
    "        .group_by('data_type')\n",
    "        .agg([\n",
    "            pl.count().alias('count'),\n",
    "            pl.col('mdrm').n_unique().alias('unique_mdrms')\n",
    "        ])\n",
    "        .sort('data_type')\n",
    "    )\n",
    "    print(type_summary)\n",
    "    \n",
    "    # 2. Integer data analysis (if available)\n",
    "    int_data = df_polars.filter(pl.col('int_data').is_not_null())\n",
    "    if len(int_data) > 0:\n",
    "        print(f\"\\nüí∞ Integer data analysis ({len(int_data)} records):\")\n",
    "        int_stats = (\n",
    "            int_data\n",
    "            .group_by(['rssd', 'quarter'])\n",
    "            .agg([\n",
    "                pl.col('int_data').min().alias('min_value'),\n",
    "                pl.col('int_data').max().alias('max_value'),\n",
    "                pl.col('int_data').mean().alias('avg_value'),\n",
    "                pl.col('int_data').count().alias('count')\n",
    "            ])\n",
    "            .sort('rssd')\n",
    "        )\n",
    "        print(int_stats)\n",
    "        \n",
    "        # Top 5 largest values\n",
    "        top_values = (\n",
    "            int_data\n",
    "            .sort('int_data', descending=True)\n",
    "            .select(['mdrm', 'rssd', 'quarter', 'int_data'])\n",
    "            .head(5)\n",
    "        )\n",
    "        print(f\"\\nüèÜ Top 5 largest integer values:\")\n",
    "        print(top_values)\n",
    "    \n",
    "    # 3. Float data analysis (if available)\n",
    "    float_data = df_polars.filter(pl.col('float_data').is_not_null())\n",
    "    if len(float_data) > 0:\n",
    "        print(f\"\\nüìà Float data analysis ({len(float_data)} records):\")\n",
    "        float_stats = (\n",
    "            float_data\n",
    "            .group_by('rssd')\n",
    "            .agg([\n",
    "                pl.col('float_data').min().alias('min_ratio'),\n",
    "                pl.col('float_data').max().alias('max_ratio'),\n",
    "                pl.col('float_data').mean().alias('avg_ratio'),\n",
    "                pl.col('float_data').count().alias('count')\n",
    "            ])\n",
    "            .sort('avg_ratio', descending=True)\n",
    "        )\n",
    "        print(float_stats)\n",
    "    \n",
    "    # 4. MDRM code frequency analysis\n",
    "    print(f\"\\nüìã Most common MDRM codes:\")\n",
    "    mdrm_freq = (\n",
    "        df_polars\n",
    "        .group_by('mdrm')\n",
    "        .agg([\n",
    "            pl.count().alias('frequency'),\n",
    "            pl.col('data_type').first().alias('data_type')\n",
    "        ])\n",
    "        .sort('frequency', descending=True)\n",
    "        .head(10)\n",
    "    )\n",
    "    print(mdrm_freq)\n",
    "    \n",
    "    # 5. Data distribution by bank and quarter\n",
    "    print(f\"\\nüè¶ Data points by bank and quarter:\")\n",
    "    bank_quarter_summary = (\n",
    "        df_polars\n",
    "        .group_by(['rssd', 'quarter'])\n",
    "        .agg([\n",
    "            pl.count().alias('total_data_points'),\n",
    "            pl.col('data_type').n_unique().alias('unique_data_types')\n",
    "        ])\n",
    "        .sort(['rssd', 'quarter'])\n",
    "    )\n",
    "    print(bank_quarter_summary)\n",
    "\n",
    "else:\n",
    "    print(\"\\nüìù This dataset uses the direct XBRL structure with type-specific columns:\")\n",
    "    print(\"   - int_data: Integer financial values\")\n",
    "    print(\"   - float_data: Ratio and percentage values\") \n",
    "    print(\"   - bool_data: Boolean indicators\")\n",
    "    print(\"   - str_data: Text values\")\n",
    "    print(\"\\nüí° This design preserves maximum type precision from the XBRL source\")\n",
    "    print(\"   and avoids the precision loss that can occur with generic 'value' columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Polars data to various formats\n",
    "print(\"üíæ Saving Polars data to files...\")\n",
    "\n",
    "# Polars CSV (very fast)\n",
    "polars_csv = 'ffiec_data_polars.csv'\n",
    "df_polars.write_csv(polars_csv)\n",
    "print(f\"‚úÖ Saved to CSV: {polars_csv}\")\n",
    "\n",
    "# Polars Parquet (recommended for large datasets)\n",
    "polars_parquet = 'ffiec_data_polars.parquet'\n",
    "df_polars.write_parquet(polars_parquet)\n",
    "print(f\"‚úÖ Saved to Parquet: {polars_parquet}\")\n",
    "\n",
    "# JSON (good for smaller datasets)\n",
    "polars_json = 'ffiec_data_polars.json'\n",
    "df_polars.write_json(polars_json)\n",
    "print(f\"‚úÖ Saved to JSON: {polars_json}\")\n",
    "\n",
    "# Delta format (if delta-rs is installed)\n",
    "try:\n",
    "    df_polars.write_delta('ffiec_data_delta')\n",
    "    print(f\"‚úÖ Saved to Delta format\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è Delta format not available: {e}\")\n",
    "\n",
    "# Display file sizes comparison\n",
    "print(\"\\nüìè File sizes comparison:\")\n",
    "polars_files = [polars_csv, polars_parquet, polars_json]\n",
    "for file in polars_files:\n",
    "    if os.path.exists(file):\n",
    "        size_mb = os.path.getsize(file) / (1024 * 1024)\n",
    "        print(f\"  {file}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Performance Comparison\n",
    "\n",
    "Compare the performance of different approaches for data collection and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison of sync vs async vs parallel\n",
    "print(\"üèÅ Performance Comparison\")\n",
    "\n",
    "# Create test data for meaningful comparison\n",
    "PERFORMANCE_BANKS = SAMPLE_BANKS[:3]  # Use first 3 banks for testing\n",
    "PERFORMANCE_PERIOD = SAMPLE_PERIODS[0]  # Use most recent period\n",
    "\n",
    "print(f\"Testing with {len(PERFORMANCE_BANKS)} banks for period {PERFORMANCE_PERIOD}\")\n",
    "\n",
    "# Mock the collect_data function for performance testing\n",
    "def mock_collect_data(session, creds, reporting_period, rssd_id, series=\"call\", **kwargs):\n",
    "    \"\"\"Mock function that simulates data collection delay.\"\"\"\n",
    "    import random\n",
    "    import time\n",
    "    \n",
    "    # Simulate network delay\n",
    "    time.sleep(random.uniform(0.1, 0.3))\n",
    "    \n",
    "    # Return mock data\n",
    "    return [{\n",
    "        'rssd_id': rssd_id,\n",
    "        'reporting_period': reporting_period,\n",
    "        'metric': 'Total Assets',\n",
    "        'value': random.randint(10000, 500000),\n",
    "        'mdrm': 'RCON2170'\n",
    "    }]\n",
    "\n",
    "performance_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test 1: Sequential (traditional approach)\nprint(\"üêå Testing sequential collection...\")\n\nstart_time = time.time()\nsequential_results = []\n\nfor i, rssd_id in enumerate(PERFORMANCE_BANKS):\n    try:\n        result = collect_data(\n            session=basic_connection.session,\n            creds=credentials,\n            reporting_period=PERFORMANCE_PERIOD,\n            rssd_id=rssd_id,\n            series=\"call\"\n        )\n    except Exception as e:\n        print(f\"Error for {rssd_id}: {e}\")\n        result = []  # Empty result on error\n    \n    sequential_results.append(result)\n    print(f\"  Completed {i+1}/{len(PERFORMANCE_BANKS)}\")\n\nsequential_time = time.time() - start_time\nperformance_results['Sequential'] = sequential_time\n\nprint(f\"‚úÖ Sequential: {sequential_time:.2f} seconds for {len(PERFORMANCE_BANKS)} requests\")\nprint(f\"   Average: {sequential_time/len(PERFORMANCE_BANKS):.2f} seconds per request\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test 2: Parallel (sync interface)\nprint(\"üöÄ Testing parallel collection...\")\n\n# Ensure sequential_time is available for comparison\nif 'sequential_time' not in locals():\n    print(\"‚ö†Ô∏è Sequential test not run yet - using estimated baseline\")\n    sequential_time = 1.0  # Fallback value\n\nwith AsyncCompatibleClient(credentials, max_concurrent=4) as client:\n    start_time = time.time()\n    \n    try:\n        parallel_results = client.collect_data_parallel(\n            reporting_period=PERFORMANCE_PERIOD,\n            rssd_ids=PERFORMANCE_BANKS,\n            series=\"call\"\n        )\n        \n        parallel_time = time.time() - start_time\n        performance_results['Parallel'] = parallel_time\n        \n        print(f\"‚úÖ Parallel: {parallel_time:.2f} seconds for {len(PERFORMANCE_BANKS)} requests\")\n        print(f\"   Average: {parallel_time/len(PERFORMANCE_BANKS):.2f} seconds per request\")\n        \n        # Only calculate speedup if we have sequential_time\n        if 'sequential_time' in locals() and sequential_time > 0:\n            speedup = sequential_time / parallel_time\n            print(f\"   Speedup: {speedup:.1f}x faster than sequential\")\n        else:\n            print(\"   (Sequential time not available for comparison)\")\n        \n    except Exception as e:\n        print(f\"‚ùå Parallel test failed: {e}\")\n        performance_results['Parallel'] = float('inf')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test 3: Pure async (for comparison)\nprint(\"‚ö° Testing pure async collection...\")\n\n# Ensure sequential_time is available for comparison\nif 'sequential_time' not in locals():\n    print(\"‚ö†Ô∏è Sequential test not run yet - using estimated baseline\")\n    sequential_time = 1.0  # Fallback value\n\nasync def test_async_performance():\n    async with AsyncCompatibleClient(\n        credentials, \n        max_concurrent=5,\n        rate_limit=20.0  # 20 requests per second\n    ) as client:\n        \n        start_time = time.time()\n        \n        # Create async tasks\n        tasks = []\n        for rssd_id in PERFORMANCE_BANKS:\n            task = client.collect_data_async(\n                reporting_period=PERFORMANCE_PERIOD,\n                rssd_id=rssd_id,\n                series=\"call\"\n            )\n            tasks.append(task)\n        \n        # Execute all tasks concurrently\n        async_results = await asyncio.gather(*tasks, return_exceptions=True)\n        \n        async_time = time.time() - start_time\n        \n        return async_time, async_results\n\ntry:\n    async_time, async_results = await test_async_performance()\n    performance_results['Async'] = async_time\n    \n    print(f\"‚úÖ Async: {async_time:.2f} seconds for {len(PERFORMANCE_BANKS)} requests\")\n    print(f\"   Average: {async_time/len(PERFORMANCE_BANKS):.2f} seconds per request\")\n    \n    # Only calculate speedup if we have sequential_time\n    if 'sequential_time' in locals() and sequential_time > 0:\n        speedup = sequential_time / async_time\n        print(f\"   Speedup: {speedup:.1f}x faster than sequential\")\n    else:\n        print(\"   (Sequential time not available for comparison)\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Async test failed: {e}\")\n    performance_results['Async'] = float('inf')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance comparison\n",
    "print(\"üìä Performance Visualization\")\n",
    "\n",
    "# Create performance comparison chart\n",
    "valid_results = {k: v for k, v in performance_results.items() if v != float('inf')}\n",
    "\n",
    "if len(valid_results) > 1:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    methods = list(valid_results.keys())\n",
    "    times = list(valid_results.values())\n",
    "    \n",
    "    # Bar chart\n",
    "    bars = plt.bar(methods, times, color=['#ff9999', '#66b3ff', '#99ff99'][:len(methods)])\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, time_val in zip(bars, times):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                f'{time_val:.2f}s', ha='center', va='bottom')\n",
    "    \n",
    "    plt.title('Performance Comparison: Data Collection Methods')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.xlabel('Collection Method')\n",
    "    \n",
    "    # Add speedup annotations\n",
    "    if 'Sequential' in valid_results:\n",
    "        baseline = valid_results['Sequential']\n",
    "        for i, (method, time_val) in enumerate(valid_results.items()):\n",
    "            if method != 'Sequential':\n",
    "                speedup = baseline / time_val\n",
    "                plt.text(i, time_val/2, f'{speedup:.1f}x\\nfaster', \n",
    "                        ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary table\n",
    "    print(\"\\nüìà Performance Summary:\")\n",
    "    num_requests = len(PERFORMANCE_BANKS)  # Use actual number of banks tested\n",
    "    perf_df = pd.DataFrame([\n",
    "        {\n",
    "            'Method': method,\n",
    "            'Total Time (s)': f\"{time_val:.2f}\",\n",
    "            'Avg per Request (s)': f\"{time_val/num_requests:.3f}\",\n",
    "            'Speedup': f\"{valid_results['Sequential']/time_val:.1f}x\" if 'Sequential' in valid_results else 'N/A'\n",
    "        }\n",
    "        for method, time_val in valid_results.items()\n",
    "    ])\n",
    "    \n",
    "    display(perf_df)\n",
    "else:\n",
    "    print(\"‚ùå Not enough valid results for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Error Handling and Validation\n",
    "\n",
    "The library provides comprehensive error handling and data validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Error handling examples\nprint(\"üõ°Ô∏è Error Handling Examples\")\n\n# 1. Invalid RSSD ID\nprint(\"\\n1. Testing invalid RSSD ID...\")\ntry:\n    invalid_data = collect_data(\n        session=basic_connection.session,\n        creds=credentials,\n        reporting_period=\"2023-12-31\",\n        rssd_id=\"invalid_id\",  # Invalid: not numeric\n        series=\"call\"\n    )\nexcept ValidationError as e:\n    print(f\"‚úÖ Caught ValidationError: {e}\")\nexcept ValueError as e:  # Legacy mode\n    print(f\"‚úÖ Caught ValueError (legacy mode): {e}\")\n\n# 2. Empty RSSD ID\nprint(\"\\n2. Testing empty RSSD ID...\")\ntry:\n    empty_data = collect_data(\n        session=basic_connection.session,\n        creds=credentials,\n        reporting_period=\"2023-12-31\",\n        rssd_id=\"\",  # Empty\n        series=\"call\"\n    )\nexcept (ValidationError, ValueError) as e:\n    print(f\"‚úÖ Caught error for empty RSSD: {e}\")\n\n# 3. Invalid credentials\nprint(\"\\n3. Testing invalid credentials...\")\ntry:\n    bad_creds = WebserviceCredentials(\"bad_user\", \"bad_pass\")\n    print(f\"Created bad credentials (masked): {bad_creds}\")\n    \n    # This would fail on actual FFIEC service\n    bad_data = collect_data(\n        session=basic_connection.session,\n        creds=bad_creds,\n        reporting_period=\"2023-12-31\",\n        rssd_id=\"480228\",\n        series=\"call\"\n    )\n        \nexcept Exception as e:\n    print(f\"‚úÖ Caught authentication error: {e}\")\n\n# 4. Invalid output type\nprint(\"\\n4. Testing invalid output type...\")\ntry:\n    bad_output = collect_data(\n        session=basic_connection.session,\n        creds=credentials,\n        reporting_period=\"2023-12-31\",\n        rssd_id=\"480228\",\n        series=\"call\",\n        output_type=\"invalid_type\"  # Invalid output type\n    )\nexcept (ValidationError, ValueError) as e:\n    print(f\"‚úÖ Caught error for invalid output type: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legacy vs New Error Mode\n",
    "print(\"üîÑ Error Mode Comparison\")\n",
    "\n",
    "from ffiec_data_connect.config import use_legacy_errors, disable_legacy_mode, enable_legacy_mode\n",
    "\n",
    "print(f\"Current legacy mode: {use_legacy_errors()}\")\n",
    "\n",
    "# Test with new error mode\n",
    "print(\"\\nüìä Testing with new error mode...\")\n",
    "disable_legacy_mode()\n",
    "try:\n",
    "    collect_data(\n",
    "        session=basic_connection.session,\n",
    "        creds=credentials,\n",
    "        reporting_period=\"2023-12-31\",\n",
    "        rssd_id=\"abc123\",\n",
    "        series=\"call\"\n",
    "    )\n",
    "except ValidationError as e:\n",
    "    print(f\"‚úÖ New mode - ValidationError: {e}\")\n",
    "    print(f\"   Error type: {type(e).__name__}\")\n",
    "    print(f\"   Field: {e.field}\")\n",
    "    print(f\"   Value: {e.value}\")\n",
    "\n",
    "# Test with legacy error mode\n",
    "print(\"\\nüîô Testing with legacy error mode...\")\n",
    "enable_legacy_mode()\n",
    "try:\n",
    "    collect_data(\n",
    "        session=basic_connection.session,\n",
    "        creds=credentials,\n",
    "        reporting_period=\"2023-12-31\",\n",
    "        rssd_id=\"abc123\",\n",
    "        series=\"call\"\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"‚úÖ Legacy mode - ValueError: {e}\")\n",
    "    print(f\"   Error type: {type(e).__name__}\")\n",
    "\n",
    "print(f\"\\nFinal legacy mode: {use_legacy_errors()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Data Visualization Examples\n",
    "\n",
    "Create visualizations from the collected financial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create visualizations from the data\nprint(\"üìä Creating Data Visualizations\")\n\n# Set up the plotting style\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n\n# Use the pandas DataFrame for plotting - work with actual data structure\nif len(df_pandas) > 0:\n    print(f\"DataFrame shape: {df_pandas.shape}\")\n    print(f\"Available columns: {df_pandas.columns.tolist()}\")\n    \n    # Check what type of data we have\n    data_types = df_pandas['data_type'].unique() if 'data_type' in df_pandas.columns else []\n    print(f\"Data types available: {data_types}\")\n    \n    # Create visualizations based on the actual data structure\n    plt.figure(figsize=(15, 10))\n    \n    # 1. Data distribution by type\n    plt.subplot(2, 2, 1)\n    if 'data_type' in df_pandas.columns:\n        type_counts = df_pandas['data_type'].value_counts()\n        plt.pie(type_counts.values, labels=type_counts.index, autopct='%1.1f%%')\n        plt.title('Distribution of Data Types')\n    else:\n        plt.text(0.5, 0.5, 'No data_type column available', ha='center', va='center')\n        plt.title('Data Type Distribution - N/A')\n    \n    # 2. Integer values analysis (if available)\n    plt.subplot(2, 2, 2)\n    int_data = df_pandas[df_pandas['int_data'].notna()] if 'int_data' in df_pandas.columns else pd.DataFrame()\n    if len(int_data) > 0:\n        # Show distribution of integer values\n        values = int_data['int_data'].astype(float)  # Convert for plotting\n        plt.hist(values, bins=20, alpha=0.7, edgecolor='black')\n        plt.title('Distribution of Integer Financial Values')\n        plt.xlabel('Value')\n        plt.ylabel('Frequency')\n        plt.ticklabel_format(style='scientific', axis='x', scilimits=(0,0))\n    else:\n        plt.text(0.5, 0.5, 'No integer data available', ha='center', va='center')\n        plt.title('Integer Data Distribution - N/A')\n    \n    # 3. Data points by bank and quarter\n    plt.subplot(2, 2, 3)\n    if 'rssd' in df_pandas.columns and 'quarter' in df_pandas.columns:\n        bank_quarter_counts = df_pandas.groupby(['rssd', 'quarter']).size().reset_index(name='count')\n        if len(bank_quarter_counts) > 0:\n            # Create a pivot table for heatmap\n            pivot_data = bank_quarter_counts.pivot(index='rssd', columns='quarter', values='count')\n            sns.heatmap(pivot_data, annot=True, fmt='d', cmap='Blues')\n            plt.title('Data Points by Bank and Quarter')\n            plt.xlabel('Reporting Quarter')\n            plt.ylabel('Bank RSSD ID')\n        else:\n            plt.text(0.5, 0.5, 'No bank/quarter data', ha='center', va='center')\n    else:\n        plt.text(0.5, 0.5, 'No bank/quarter columns available', ha='center', va='center')\n        plt.title('Bank/Quarter Analysis - N/A')\n    \n    # 4. MDRM code frequency\n    plt.subplot(2, 2, 4)\n    if 'mdrm' in df_pandas.columns:\n        mdrm_counts = df_pandas['mdrm'].value_counts().head(10)\n        if len(mdrm_counts) > 0:\n            plt.barh(range(len(mdrm_counts)), mdrm_counts.values)\n            plt.yticks(range(len(mdrm_counts)), mdrm_counts.index)\n            plt.title('Top 10 Most Common MDRM Codes')\n            plt.xlabel('Frequency')\n            plt.gca().invert_yaxis()  # Highest at top\n        else:\n            plt.text(0.5, 0.5, 'No MDRM data available', ha='center', va='center')\n    else:\n        plt.text(0.5, 0.5, 'No MDRM column available', ha='center', va='center')\n        plt.title('MDRM Code Analysis - N/A')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Save the visualization\n    plt.savefig('ffiec_data_visualization.png', dpi=300, bbox_inches='tight')\n    print(\"‚úÖ Visualization saved as 'ffiec_data_visualization.png'\")\n    \n    # Print summary of what we visualized\n    print(f\"\\nüìà Visualization Summary:\")\n    print(f\"   ‚Ä¢ Data types: {len(data_types)} unique types\")\n    print(f\"   ‚Ä¢ Integer values: {len(int_data)} records\" if 'int_data' in df_pandas.columns else \"   ‚Ä¢ No integer data\")\n    \n    float_data_count = len(df_pandas[df_pandas['float_data'].notna()]) if 'float_data' in df_pandas.columns else 0\n    print(f\"   ‚Ä¢ Float values: {float_data_count} records\")\n    \n    banks = df_pandas['rssd'].nunique() if 'rssd' in df_pandas.columns else 0\n    quarters = df_pandas['quarter'].nunique() if 'quarter' in df_pandas.columns else 0\n    print(f\"   ‚Ä¢ Banks: {banks}, Quarters: {quarters}\")\n    \nelse:\n    print(\"‚ùå No data available for visualization\")\n    print(\"This might happen if:\")\n    print(\"   ‚Ä¢ The data collection failed\")\n    print(\"   ‚Ä¢ You're using demo/mock data\")\n    print(\"   ‚Ä¢ Network connectivity issues\")\n    \n    # Create a simple demo visualization\n    print(\"\\nüìä Creating demo visualization instead...\")\n    \n    plt.figure(figsize=(8, 6))\n    demo_data = {\n        'int_data': [1000000, 2000000, 1500000],\n        'float_data': [12.5, 15.2, 13.8], \n        'data_type': ['int', 'int', 'float']\n    }\n    demo_df = pd.DataFrame(demo_data)\n    \n    plt.subplot(1, 2, 1)\n    plt.bar(['Bank A', 'Bank B', 'Bank C'], demo_df['int_data'].fillna(0))\n    plt.title('Demo: Asset Values by Bank')\n    plt.ylabel('Assets ($)')\n    plt.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n    \n    plt.subplot(1, 2, 2)\n    plt.pie(demo_df['data_type'].value_counts().values, \n           labels=demo_df['data_type'].value_counts().index, \n           autopct='%1.1f%%')\n    plt.title('Demo: Data Type Distribution')\n    \n    plt.tight_layout()\n    plt.show()\n    print(\"‚úÖ Demo visualization created\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleanup and Summary\n",
    "\n",
    "Clean up resources and summarize what we've accomplished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cleanup resources\nprint(\"üßπ Cleaning up resources...\")\n\n# Close connections\nif 'basic_connection' in locals():\n    basic_connection.close()\n    print(\"‚úÖ Closed basic connection\")\n\nif 'async_client' in locals():\n    async_client.close()\n    print(\"‚úÖ Closed async client\")\n\n# Clear SOAP cache\nfrom ffiec_data_connect import clear_soap_cache, get_cache_stats\n\nprint(f\"\\nSOAP cache stats before cleanup: {get_cache_stats()}\")\nclear_soap_cache()\nprint(f\"SOAP cache stats after cleanup: {get_cache_stats()}\")\n\nprint(\"\\nüéâ Cleanup completed!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of demonstration\n",
    "print(\"üìã DEMONSTRATION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nüîç What we demonstrated:\")\n",
    "print(\"  ‚úÖ Basic synchronous data collection\")\n",
    "print(\"  ‚úÖ Async data collection with rate limiting\")\n",
    "print(\"  ‚úÖ Parallel processing with sync interface\")\n",
    "print(\"  ‚úÖ Pandas DataFrame creation and analysis\")\n",
    "print(\"  ‚úÖ Polars DataFrame operations and performance\")\n",
    "print(\"  ‚úÖ Data export to multiple formats (CSV, Excel, Parquet, JSON)\")\n",
    "print(\"  ‚úÖ Performance comparison of different approaches\")\n",
    "print(\"  ‚úÖ Comprehensive error handling and validation\")\n",
    "print(\"  ‚úÖ Data visualization with matplotlib/seaborn\")\n",
    "print(\"  ‚úÖ Resource management and cleanup\")\n",
    "\n",
    "print(\"\\nüíæ Files created:\")\n",
    "created_files = [\n",
    "    'ffiec_data_pandas.csv',\n",
    "    'ffiec_data_pandas.xlsx', \n",
    "    'ffiec_data_pandas.parquet',\n",
    "    'ffiec_data_polars.csv',\n",
    "    'ffiec_data_polars.parquet',\n",
    "    'ffiec_data_polars.json',\n",
    "    'ffiec_data_visualization.png'\n",
    "]\n",
    "\n",
    "total_size = 0\n",
    "for file in created_files:\n",
    "    if os.path.exists(file):\n",
    "        size_kb = os.path.getsize(file) / 1024\n",
    "        total_size += size_kb\n",
    "        print(f\"  üìÑ {file}: {size_kb:.1f} KB\")\n",
    "\n",
    "print(f\"\\nüìä Total files size: {total_size:.1f} KB\")\n",
    "\n",
    "if 'performance_results' in locals():\n",
    "    print(\"\\n‚ö° Performance highlights:\")\n",
    "    for method, time_val in performance_results.items():\n",
    "        if time_val != float('inf'):\n",
    "            print(f\"  {method}: {time_val:.2f} seconds\")\n",
    "\n",
    "print(\"\\nüöÄ Ready for production use!\")\n",
    "print(\"\\nüí° Next steps:\")\n",
    "print(\"  1. Set up your FFIEC credentials as environment variables\")\n",
    "print(\"  2. Use AsyncCompatibleClient for better performance\")\n",
    "print(\"  3. Choose Polars for large datasets, Pandas for smaller ones\")\n",
    "print(\"  4. Implement proper error handling in your applications\")\n",
    "print(\"  5. Use context managers for automatic resource cleanup\")\n",
    "\n",
    "print(\"\\nüìö Documentation: https://ffiec-data-connect.readthedocs.io/\")\n",
    "print(\"üêõ Report issues: https://github.com/call-report/ffiec-data-connect/issues\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}